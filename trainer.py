# -*- coding: utf-8 -*-
"""
è®­ç»ƒå™¨æ¨¡å— (trainer.py)

åŠŸèƒ½è¯´æ˜:
    ç®¡ç†å®Œæ•´çš„è®­ç»ƒæµç¨‹ï¼Œé›†æˆOptunaè¶…å‚æ•°ä¼˜åŒ–å’ŒWandBå®éªŒè¿½è¸ª

ä¸»è¦å†…å®¹:
    - EarlyStopping: æ—©åœæœºåˆ¶ç±»
        * ç›‘æ§éªŒè¯æŸå¤±å˜åŒ–
        * é¿å…è¿‡æ‹Ÿåˆ
    - TrainingLogger: è®­ç»ƒæ—¥å¿—è®°å½•å™¨
        * æœ¬åœ°æ—¥å¿—ä¿å­˜
        * WandBå®éªŒè¿½è¸ªé›†æˆ
        * å®æ—¶æŒ‡æ ‡è®°å½•
    - Trainer: ä¸»è®­ç»ƒå™¨ç±»
        * å®Œæ•´è®­ç»ƒå¾ªç¯ç®¡ç†
        * ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨åˆ›å»º
        * æ¨¡å‹ä¿å­˜å’ŒåŠ è½½
        * Optunaå‰ªææ”¯æŒ
        * WandBå¯è§†åŒ–é›†æˆ
    - create_hyperparameter_study: Optunaè¶…å‚æ•°ä¼˜åŒ–
        * è‡ªåŠ¨æœç´¢æœ€ä¼˜è¶…å‚æ•°
        * æ”¯æŒå¹¶è¡Œä¼˜åŒ–
        * æ™ºèƒ½å‰ªææœºåˆ¶
    - create_trial_config: ä¸ºä¼˜åŒ–è¯•éªŒåˆ›å»ºé…ç½®

è®­ç»ƒæµç¨‹:
    1. æ•°æ®åŠ è½½ -> 2. æ¨¡å‹å‰å‘ -> 3. æŸå¤±è®¡ç®— -> 4. åå‘ä¼ æ’­
    5. å‚æ•°æ›´æ–° -> 6. éªŒè¯è¯„ä¼° -> 7. æ—©åœæ£€æŸ¥ -> 8. æ¨¡å‹ä¿å­˜

é›†æˆåŠŸèƒ½:
    - WandB: å®æ—¶ç›‘æ§ã€å®éªŒå¯¹æ¯”ã€æ¨¡å‹ç®¡ç†
    - Optuna: è¶…å‚æ•°ä¼˜åŒ–ã€è¯•éªŒå‰ªæã€ç»“æœåˆ†æ

ä½¿ç”¨æ–¹æ³•:
    # æ™®é€šè®­ç»ƒ
    trainer = Trainer(model, train_loader, val_loader, config, use_wandb=True)
    trainer.train()
    
    # è¶…å‚æ•°ä¼˜åŒ–
    study = create_hyperparameter_study(config, model_factory, data_factory)

ä½œè€…: åŸºäºSwinFuseé¡¹ç›®é‡æ„
æ—¥æœŸ: 2025å¹´9æœˆ
"""

import os
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Adam, AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR
from tqdm import tqdm
import numpy as np
from typing import Dict, List, Tuple, Optional

from losses import create_loss_function, compute_feature_similarity, CombinedLoss

# å¯é€‰ä¾èµ–å¯¼å…¥
try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False
    print("âš ï¸ WandBæœªå®‰è£…ï¼Œå¯è§†åŒ–åŠŸèƒ½å°†è¢«ç¦ç”¨")

try:
    import optuna
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False
    print("âš ï¸ Optunaæœªå®‰è£…ï¼Œè¶…å‚æ•°ä¼˜åŒ–åŠŸèƒ½å°†è¢«ç¦ç”¨")


class EarlyStopping:
    """æ—©åœç±»"""
    
    def __init__(self, patience: int = 10, min_delta: float = 1e-6):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = float('inf')
        
    def __call__(self, val_loss: float) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ—©åœ"""
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
        else:
            self.counter += 1
            
        return self.counter >= self.patience


class TrainingLogger:
    """è®­ç»ƒæ—¥å¿—è®°å½•å™¨ - é›†æˆWandBæ”¯æŒ"""
    
    def __init__(self, log_dir: str, use_wandb: bool = False, project_name: str = "SwinFuse-FeatureExtractor"):
        self.log_dir = log_dir
        self.use_wandb = use_wandb and WANDB_AVAILABLE
        os.makedirs(log_dir, exist_ok=True)
        
        # è®°å½•åˆ—è¡¨
        self.train_losses = []
        self.val_losses = []
        self.loss_components = {'infonce': [], 'coral': [], 'barlow': []}
        self.similarities = []
        self.learning_rates = []
        
        # åˆå§‹åŒ–WandB
        if self.use_wandb:
            try:
                wandb.init(
                    project=project_name,
                    name=f"swinfuse_{int(time.time())}",
                    tags=["feature-extraction", "contrastive-learning", "ir-visible"]
                )
                print("âœ… WandBåˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ WandBåˆå§‹åŒ–å¤±è´¥: {e}")
                self.use_wandb = False
        
    def log_config(self, config):
        """è®°å½•é…ç½®åˆ°WandB"""
        if self.use_wandb:
            wandb.config.update({
                "learning_rate": config.training.learning_rate,
                "batch_size": config.training.batch_size,
                "num_epochs": config.training.num_epochs,
                "weight_decay": config.training.weight_decay,
                "scheduler_type": config.training.scheduler_type,
                "projection_input_dim": config.model.projection_input_dim,
                "projection_hidden_dim": config.model.projection_hidden_dim,
                "projection_output_dim": config.model.projection_output_dim,
                "loss_type": config.loss.loss_type,
                "temperature": config.loss.temperature,
                "lambda_coral": config.loss.lambda_coral,
                "lambda_barlow": config.loss.lambda_barlow,
                "image_size": config.data.image_size,
                "use_augmentation": config.data.use_augmentation,
                "device": config.device
            })
    
    def log_epoch(self, 
                  epoch: int,
                  train_loss: float,
                  val_loss: float,
                  loss_components: Optional[Dict[str, float]] = None,
                  similarity: Optional[float] = None,
                  lr: Optional[float] = None):
        """è®°å½•ä¸€ä¸ªepochçš„ä¿¡æ¯"""
        self.train_losses.append(train_loss)
        self.val_losses.append(val_loss)
        
        # å‡†å¤‡WandBæ—¥å¿—
        wandb_metrics = {
            "epoch": epoch,
            "train_loss": train_loss,
            "val_loss": val_loss,
            "learning_rate": lr if lr is not None else 0.0
        }
        
        if loss_components:
            for key in self.loss_components:
                if key in loss_components:
                    self.loss_components[key].append(loss_components[key])
                    wandb_metrics[f"train_{key}"] = loss_components[key]
        
        if similarity is not None:
            self.similarities.append(similarity)
            wandb_metrics["similarity"] = similarity
            
        if lr is not None:
            self.learning_rates.append(lr)
        
        # è®°å½•åˆ°WandB
        if self.use_wandb:
            wandb.log(wandb_metrics, step=epoch)
    
    def finish(self):
        """ç»“æŸWandBè¿è¡Œ"""
        if self.use_wandb:
            wandb.finish()
    
    def save_logs(self):
        """ä¿å­˜æ—¥å¿—åˆ°æ–‡ä»¶"""
        log_file = os.path.join(self.log_dir, 'training_log.txt')
        with open(log_file, 'w', encoding='utf-8') as f:
            f.write("Epoch,Train_Loss,Val_Loss,InfoNCE,CORAL,Barlow,Similarity,LR\n")
            for i in range(len(self.train_losses)):
                line = f"{i+1},{self.train_losses[i]:.6f},{self.val_losses[i]:.6f}"
                
                # æŸå¤±ç»„ä»¶
                for key in ['infonce', 'coral', 'barlow']:
                    if i < len(self.loss_components[key]):
                        line += f",{self.loss_components[key][i]:.6f}"
                    else:
                        line += ",0.0"
                
                # ç›¸ä¼¼åº¦å’Œå­¦ä¹ ç‡
                if i < len(self.similarities):
                    line += f",{self.similarities[i]:.6f}"
                else:
                    line += ",0.0"
                    
                if i < len(self.learning_rates):
                    line += f",{self.learning_rates[i]:.2e}"
                else:
                    line += ",0.0"
                
                f.write(line + "\n")
    
    def get_best_epoch(self) -> int:
        """è·å–æœ€ä½³epoch"""
        if not self.val_losses:
            return 0
        return np.argmin(self.val_losses) + 1


class Trainer:
    """è®­ç»ƒå™¨ç±» - é›†æˆOptunaå’ŒWandBæ”¯æŒ"""
    
    def __init__(self, 
                 model: nn.Module,
                 train_loader,
                 val_loader,
                 config,
                 use_wandb: bool = False,
                 trial: Optional['optuna.Trial'] = None):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.config = config
        self.device = torch.device(config.device)
        self.trial = trial  # Optunaè¯•éªŒå¯¹è±¡
        
        # åˆ›å»ºæŸå¤±å‡½æ•°
        self.criterion = create_loss_function(config)
        self.criterion.to(self.device)
        
        # åˆ›å»ºä¼˜åŒ–å™¨
        self.optimizer = self._create_optimizer()
        
        # åˆ›å»ºè°ƒåº¦å™¨
        self.scheduler = self._create_scheduler()
        
        # æ—©åœ
        self.early_stopping = None
        if config.training.early_stopping:
            self.early_stopping = EarlyStopping(
                patience=config.training.patience,
                min_delta=config.training.min_delta
            )
        
        # æ—¥å¿—è®°å½• - é›†æˆWandB
        self.logger = TrainingLogger(
            config.paths.log_dir, 
            use_wandb=use_wandb and not trial  # åœ¨Optunaè¯•éªŒä¸­ä¸ä½¿ç”¨WandBé¿å…å†²çª
        )
        
        # è®°å½•é…ç½®åˆ°WandB
        if use_wandb and not trial:
            self.logger.log_config(config)
        
        # æœ€ä½³æ¨¡å‹è®°å½•
        self.best_val_loss = float('inf')
        self.best_model_path = os.path.join(config.paths.checkpoint_dir, 'best_model.pth')
        
        print("è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _create_optimizer(self):
        """åˆ›å»ºä¼˜åŒ–å™¨"""
        return Adam(
            self.model.parameters(),
            lr=self.config.training.learning_rate,
            weight_decay=self.config.training.weight_decay
        )
    
    def _create_scheduler(self):
        """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
        if self.config.training.scheduler_type == "cosine":
            return CosineAnnealingLR(
                self.optimizer,
                T_max=self.config.training.num_epochs,
                eta_min=self.config.training.min_lr
            )
        elif self.config.training.scheduler_type == "step":
            return StepLR(self.optimizer, step_size=10, gamma=0.5)
        else:
            return None
    
    def train_epoch(self) -> Tuple[float, Dict[str, float]]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        epoch_losses = []
        epoch_components = []
        
        progress_bar = tqdm(self.train_loader, desc='è®­ç»ƒ')
        
        for batch_idx, (ir_batch, vis_batch, _) in enumerate(progress_bar):
            # ç§»åŠ¨åˆ°è®¾å¤‡
            ir_batch = ir_batch.to(self.device, non_blocking=True)
            vis_batch = vis_batch.to(self.device, non_blocking=True)
            
            # å‰å‘ä¼ æ’­
            self.optimizer.zero_grad()
            
            ir_proj = self.model(ir_batch)
            vis_proj = self.model(vis_batch)
            
            # è®¡ç®—æŸå¤±
            if isinstance(self.criterion, CombinedLoss):
                loss, loss_dict = self.criterion(ir_proj, vis_proj)
                epoch_components.append(loss_dict)
            else:
                loss = self.criterion(ir_proj, vis_proj)
                loss_dict = {'total': loss.item()}
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            if self.config.training.max_grad_norm > 0:
                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(), 
                    self.config.training.max_grad_norm
                )
            
            self.optimizer.step()
            
            epoch_losses.append(loss.item())
            
            # æ›´æ–°è¿›åº¦æ¡
            if isinstance(self.criterion, CombinedLoss):
                progress_bar.set_postfix({
                    'Loss': f'{loss.item():.6f}',
                    'InfoNCE': f'{loss_dict["infonce"]:.6f}',
                    'CORAL': f'{loss_dict["coral"]:.6f}',
                    'Barlow': f'{loss_dict["barlow"]:.6f}'
                })
            else:
                progress_bar.set_postfix({'Loss': f'{loss.item():.6f}'})
        
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_loss = np.mean(epoch_losses)
        
        # è®¡ç®—å¹³å‡æŸå¤±ç»„ä»¶
        avg_components = {}
        if epoch_components:
            for key in epoch_components[0].keys():
                avg_components[key] = np.mean([d[key] for d in epoch_components])
        
        return avg_loss, avg_components
    
    def validate_epoch(self) -> Tuple[float, float]:
        """éªŒè¯ä¸€ä¸ªepoch"""
        self.model.eval()
        
        val_losses = []
        all_ir_proj = []
        all_vis_proj = []
        
        with torch.no_grad():
            progress_bar = tqdm(self.val_loader, desc='éªŒè¯')
            
            for ir_batch, vis_batch, _ in progress_bar:
                ir_batch = ir_batch.to(self.device, non_blocking=True)
                vis_batch = vis_batch.to(self.device, non_blocking=True)
                
                ir_proj = self.model(ir_batch)
                vis_proj = self.model(vis_batch)
                
                # è®¡ç®—æŸå¤±
                if isinstance(self.criterion, CombinedLoss):
                    loss, _ = self.criterion(ir_proj, vis_proj)
                else:
                    loss = self.criterion(ir_proj, vis_proj)
                
                val_losses.append(loss.item())
                
                # æ”¶é›†ç‰¹å¾ç”¨äºç›¸ä¼¼åº¦è®¡ç®—
                all_ir_proj.append(ir_proj.cpu())
                all_vis_proj.append(vis_proj.cpu())
                
                progress_bar.set_postfix({'Val Loss': f'{loss.item():.6f}'})
        
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_val_loss = np.mean(val_losses)
        
        # è®¡ç®—ç‰¹å¾ç›¸ä¼¼åº¦
        all_ir_proj = torch.cat(all_ir_proj, dim=0)
        all_vis_proj = torch.cat(all_vis_proj, dim=0)
        similarity = compute_feature_similarity(all_ir_proj, all_vis_proj)
        
        return avg_val_loss, similarity
    
    def save_model(self, path: str, epoch: int, is_best: bool = False):
        """ä¿å­˜æ¨¡å‹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'val_loss': self.best_val_loss,
            'config': self.config
        }
        
        if self.scheduler:
            checkpoint['scheduler_state_dict'] = self.scheduler.state_dict()
        
        torch.save(checkpoint, path)
        
        if is_best:
            print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹: {path}")
        else:
            print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {path}")
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯ - é›†æˆOptunaå‰ªæå’ŒWandBè®°å½•"""
        print("\n" + "=" * 60)
        print("å¼€å§‹è®­ç»ƒ")
        print("=" * 60)
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"WandBçŠ¶æ€: {'âœ…å¯ç”¨' if self.logger.use_wandb else 'âŒç¦ç”¨'}")
        print(f"Optunaå‰ªæ: {'âœ…å¯ç”¨' if self.trial else 'âŒç¦ç”¨'}")
        
        start_time = time.time()
        
        for epoch in range(self.config.training.num_epochs):
            epoch_start = time.time()
            
            # è®­ç»ƒé˜¶æ®µ
            train_loss, train_components = self.train_epoch()
            
            # éªŒè¯é˜¶æ®µ
            val_loss, similarity = self.validate_epoch()
            
            # æ›´æ–°å­¦ä¹ ç‡
            current_lr = self.optimizer.param_groups[0]['lr']
            if self.scheduler:
                self.scheduler.step()
            
            # è®°å½•æ—¥å¿—
            self.logger.log_epoch(
                epoch + 1, train_loss, val_loss, 
                train_components, similarity, current_lr
            )
            
            # Optunaå‰ªææ£€æŸ¥
            if self.trial is not None:
                self.trial.report(val_loss, epoch)
                if self.trial.should_prune():
                    print(f"\nâœ‚ï¸ Optunaå‰ªæè§¦å‘ï¼šç¬¬{epoch+1}è½®åœæ­¢è¯•éªŒ")
                    self.logger.finish()
                    raise optuna.TrialPruned()
            
            # æ‰“å°ç»“æœ
            epoch_time = time.time() - epoch_start
            print(f"\nEpoch {epoch+1}/{self.config.training.num_epochs} (è€—æ—¶: {epoch_time:.1f}s)")
            print(f"  è®­ç»ƒæŸå¤±: {train_loss:.6f}")
            print(f"  éªŒè¯æŸå¤±: {val_loss:.6f}")
            print(f"  ç‰¹å¾ç›¸ä¼¼åº¦: {similarity:.4f}")
            print(f"  å­¦ä¹ ç‡: {current_lr:.2e}")
            
            if train_components:
                print(f"  InfoNCE: {train_components['infonce']:.6f}")
                print(f"  CORAL: {train_components['coral']:.6f}")
                print(f"  Barlow: {train_components['barlow']:.6f}")
            
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.save_model(self.best_model_path, epoch + 1, is_best=True)
                
                # è®°å½•æœ€ä½³æ¨¡å‹åˆ°WandB
                if self.logger.use_wandb:
                    wandb.log({"best_val_loss": val_loss}, step=epoch+1)
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if (epoch + 1) % self.config.save_interval == 0:
                checkpoint_path = os.path.join(
                    self.config.paths.checkpoint_dir, 
                    f'checkpoint_epoch_{epoch+1}.pth'
                )
                self.save_model(checkpoint_path, epoch + 1)
            
            # æ—©åœæ£€æŸ¥
            if self.early_stopping and self.early_stopping(val_loss):
                print(f"\nâ¹ï¸ æ—©åœè§¦å‘ï¼šéªŒè¯æŸå¤±è¿ç»­{self.config.training.patience}è½®æœªæ”¹å–„")
                break
        
        # è®­ç»ƒç»“æŸ
        total_time = time.time() - start_time
        print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print(f"æ€»è€—æ—¶: {total_time/3600:.1f} å°æ—¶")
        print(f"æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.6f}")
        
        # ç»“æŸWandBè¿è¡Œ
        self.logger.finish()
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹å’Œæ—¥å¿—
        final_model_path = os.path.join(self.config.paths.checkpoint_dir, 'final_model.pth')
        self.save_model(final_model_path, self.config.training.num_epochs)
        self.logger.save_logs()
        
        return self.best_val_loss


def create_hyperparameter_study(config, model_factory, data_loaders_factory, n_trials: int = 50):
    """åˆ›å»ºOptunaè¶…å‚æ•°ä¼˜åŒ–ç ”ç©¶"""
    if not OPTUNA_AVAILABLE:
        raise ImportError("éœ€è¦å®‰è£…Optuna: pip install optuna")
    
    def suggest_hyperparameters(trial):
        """å»ºè®®è¶…å‚æ•°"""
        return {
            # å­¦ä¹ ç‡
            "learning_rate": trial.suggest_float("learning_rate", 1e-6, 1e-3, log=True),
            
            # æ‰¹æ¬¡å¤§å°
            "batch_size": trial.suggest_categorical("batch_size", [4, 8, 16, 32]),
            
            # æƒé‡è¡°å‡
            "weight_decay": trial.suggest_float("weight_decay", 1e-6, 1e-3, log=True),
            
            # æ¸©åº¦å‚æ•°
            "temperature": trial.suggest_float("temperature", 0.01, 0.2),
            
            # æŸå¤±æƒé‡
            "lambda_coral": trial.suggest_float("lambda_coral", 0.001, 0.1, log=True),
            "lambda_barlow": trial.suggest_float("lambda_barlow", 0.001, 0.1, log=True),
            
            # æŠ•å½±å¤´é…ç½®
            "projection_hidden_dim": trial.suggest_categorical("projection_hidden_dim", [128, 256, 512]),
            "projection_output_dim": trial.suggest_categorical("projection_output_dim", [64, 128, 256]),
            
            # è°ƒåº¦å™¨å‚æ•°
            "scheduler_type": trial.suggest_categorical("scheduler_type", ["cosine", "step"]),
            "warmup_epochs": trial.suggest_int("warmup_epochs", 1, 5),
        }
    
    def objective(trial):
        """ä¼˜åŒ–ç›®æ ‡å‡½æ•°"""
        # è·å–å»ºè®®çš„è¶…å‚æ•°
        hyperparams = suggest_hyperparameters(trial)
        
        # åˆ›å»ºé…ç½®å‰¯æœ¬å¹¶æ›´æ–°è¶…å‚æ•°
        trial_config = create_trial_config(config, hyperparams)
        
        # åˆ›å»ºæ¨¡å‹å’Œæ•°æ®åŠ è½½å™¨
        model = model_factory(trial_config)
        train_loader, val_loader = data_loaders_factory(trial_config)
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = Trainer(
            model=model,
            train_loader=train_loader,
            val_loader=val_loader,
            config=trial_config,
            use_wandb=False,  # åœ¨Optunaä¸­ä¸ä½¿ç”¨WandBé¿å…å†²çª
            trial=trial
        )
        
        # è®­ç»ƒæ¨¡å‹
        try:
            best_val_loss = trainer.train()
            return best_val_loss
        except Exception as e:
            print(f"Trial {trial.number} å¤±è´¥: {e}")
            raise optuna.TrialPruned()
    
    # åˆ›å»ºç ”ç©¶
    study = optuna.create_study(direction="minimize")
    
    # æ‰§è¡Œä¼˜åŒ–
    study.optimize(objective, n_trials=n_trials)
    
    # è¾“å‡ºç»“æœ
    print("=" * 60)
    print("ğŸ¯ è¶…å‚æ•°ä¼˜åŒ–å®Œæˆ")
    print("=" * 60)
    print(f"æœ€ä½³è¯•éªŒ: {study.best_trial.number}")
    print(f"æœ€ä½³éªŒè¯æŸå¤±: {study.best_value:.6f}")
    print("æœ€ä½³è¶…å‚æ•°:")
    for key, value in study.best_params.items():
        print(f"  {key}: {value}")
    
    return study


def create_trial_config(base_config, hyperparams):
    """ä¸ºè¯•éªŒåˆ›å»ºé…ç½®"""
    from config import Config
    
    config = Config()
    
    # å¤åˆ¶åŸºç¡€é…ç½®
    config.training = base_config.training
    config.data = base_config.data
    config.model = base_config.model
    config.loss = base_config.loss
    config.paths = base_config.paths
    config.device = base_config.device
    
    # åº”ç”¨è¶…å‚æ•°
    config.training.learning_rate = hyperparams["learning_rate"]
    config.training.batch_size = hyperparams["batch_size"]
    config.training.weight_decay = hyperparams["weight_decay"]
    config.training.scheduler_type = hyperparams["scheduler_type"]
    config.training.warmup_epochs = hyperparams["warmup_epochs"]
    
    config.loss.temperature = hyperparams["temperature"]
    config.loss.lambda_coral = hyperparams["lambda_coral"]
    config.loss.lambda_barlow = hyperparams["lambda_barlow"]
    
    config.model.projection_hidden_dim = hyperparams["projection_hidden_dim"]
    config.model.projection_output_dim = hyperparams["projection_output_dim"]
    
    return config


if __name__ == "__main__":
    # æµ‹è¯•è®­ç»ƒå™¨
    from config import get_config
    from data_loader import create_dataloaders
    from models import create_feature_extractor
    
    config = get_config()
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader, val_loader = create_dataloaders(config)
    
    # åˆ›å»ºæ¨¡å‹
    model = create_feature_extractor(config, config.device)
    
    if model is not None:
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = Trainer(model, train_loader, val_loader, config)
        
        # å¼€å§‹è®­ç»ƒï¼ˆæµ‹è¯•ä¸€ä¸ªepochï¼‰
        config.training.num_epochs = 1
        trainer.train()
